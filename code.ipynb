{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Pobranie z GitHub repozytorium Morphence","metadata":{"id":"OkZ9294yK3iE"}},{"cell_type":"code","source":"!git clone https://github.com/um-dsp/Morphence.git\n!git checkout 0a10954\n%cd Morphence\n!pip install -r requirements.txt\n!rm CNN_MNIST.pth","metadata":{"id":"OO6AodulK3iJ","execution":{"iopub.status.busy":"2022-06-13T09:18:19.369828Z","iopub.execute_input":"2022-06-13T09:18:19.370992Z","iopub.status.idle":"2022-06-13T09:20:23.985040Z","shell.execute_reply.started":"2022-06-13T09:18:19.370852Z","shell.execute_reply":"2022-06-13T09:20:23.983587Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Funkcje użyte podczas badań","metadata":{"id":"npsLxwilK3iL"}},{"cell_type":"code","source":"import os\nimport pickle\nimport numpy as np\nimport time\nimport torch\nimport torchvision\nfrom tqdm import tqdm\nfrom absl import app, flags\nfrom easydict import EasyDict\nfrom random import random\nfrom cleverhans.torch.attacks.projected_gradient_descent import (projected_gradient_descent,)\nfrom cleverhans.torch.attacks.carlini_wagner_l2 import carlini_wagner_l2\nfrom cleverhans.torch.attacks.spsa import spsa\nfrom cleverhans.torch.attacks.fast_gradient_method import fast_gradient_method\nfrom cifar import get_datasets, CNN\n\n# Get current working directory\ncwd = os.getcwd()\n\n# Create execution device\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ntorch.backends.cudnn.benchmark = True\n\nsupported_datasets = ['MNIST','CIFAR10']\nsupported_attacks = ['NoAttack','CW', 'FGS', 'SPSA']\n\n    \ndef load_data(train=True,test=True):\n    ''' load dataset'''\n    \n    if flag_data=='MNIST':\n        data = ld_mnist(batch_size=flag_batch)\n    elif flag_data=='CIFAR10':\n        data = get_datasets(root=os.path.join(cwd,'Copycat','Framework','data'),train=train, batch=flag_batch)\n    elif flag_data not in supported_datasets:\n        raise ValueError('Dataset {} is not supported'.format(flag_data))\n\n    return data\n\ndef load_model(dir_path,ind,adv_train=False,model_type='mtd',copycat=False):\n    '''load model'''\n    \n    \n    if flag_data not in supported_datasets:\n        raise ValueError('Dataset {} is not supported'.format(flag_data))\n    \n    if flag_data == 'MNIST':\n        model = Model4k()\n    elif flag_data == 'CIFAR10':\n        model = CNN()\n    \n    model.cuda()\n    if adv_train==False:\n        if copycat==False:\n            model.load_state_dict(torch.load(os.path.join(cwd,dir_path,\"CNN_\"+flag_data + str(ind) + \".pth\")))\n        else:\n            model.load_state_dict(torch.load(os.path.join(cwd,dir_path,'copycat_'+model_type+'_'+\"CNN_\"+flag_data + str(ind) + \".pth\")))\n    else:\n        if copycat==False:\n            model.load_state_dict(torch.load(os.path.join(cwd,dir_path,\"R-CNN_\"+flag_data + str(ind) + \".pth\")))\n        else:\n            model.load_state_dict(torch.load(os.path.join(cwd,dir_path,'copycat_'+model_type+'_'+\"R-CNN_\"+flag_data + str(ind) + \".pth\")))\n    \n    return model\n        \n    \n        \ndef accuracy(model,data, size, model_type='torch'):\n    '''compute accuracy'''\n    \n    if model_type=='torch':\n        if device == \"cuda\":\n            model = model.cuda()\n        model.eval()\n        \n    shape=0\n    report = EasyDict(nb_test=0, correct=0)\n    for x, y in data.test:\n        x, y = x.to(device), y.to(device)\n        _, y_pred = model(x).max(1)\n        report.nb_test += y.size(0)\n        report.correct += y_pred.eq(y).sum().item()\n        shape+=x.shape[0]\n        if model_type=='mtd':\n            print('Current accuracy is :{} %'.format(report.correct / report.nb_test * 100.0))\n        if shape >= size:\n            break\n            \n    return report.correct / report.nb_test * 100.0\n\n\ndef perform_attack(model,data,size,attack='spsa',model_type='mtd',copycat=False):\n    '''perform a cleverhans attack\n    \n    model_type: can be either mtd or master or master_adv'''\n    i=0\n    correct=0\n    nb_test=0\n    for x, y in data.test:\n        #print(x.shape)\n        x, y = x.to(device), y.to(device)\n        print('# Performing {} attack on batch {}'.format(attack,i+1))\n        if attack=='spsa':\n            x = spsa(model, x,eps=flag_eps,nb_iter=1000,norm = np.inf,sanity_checks=False)\n        if attack=='CW':\n            x = carlini_wagner_l2(model, x, 10,y,targeted = False)\n            \n        if attack=='FGS':\n            x = fast_gradient_method(model, x, eps=flag_eps, norm = np.inf)\n        \n        if model_type=='mtd':\n            if attack != 'spsa':\n                model.eval()\n        _, y_pred = model(x).max(1)\n        nb_test += x.shape[0]\n        correct += y_pred.eq(y).sum().item()\n        print('Current robustness against {} is :{} %'.format(attack,correct/nb_test *100))\n        # save adv test data\n        if i==0:\n            x_adv = x\n            y_adv = y\n        else:\n            x_adv = torch.cat((x_adv,x))\n            y_adv = torch.cat((y_adv,y))\n        i+=1\n        print(x_adv.shape)\n        print(y_adv.shape)\n        if x_adv.shape[0]>=size:\n            break\n        \n    if model_type=='master':\n        if copycat==False:\n            dir_path = os.path.join(cwd,attack+'_test_master')\n        else:\n            dir_path = os.path.join(cwd,attack+'copycat_test_master')\n        if not os.path.exists(dir_path):\n            os.makedirs(dir_path)\n        f = open(os.path.join(cwd,attack+'_test_master',flag_data), 'wb')\n        pickle.dump((x_adv,y_adv), f)\n        f.close()\n    \n    if model_type=='master_adv':\n        if copycat==False:\n            dir_path = os.path.join(cwd,attack+'_test_master_adv')\n        else:\n            dir_path = os.path.join(cwd,attack+'copycat_test_master_adv')\n        if not os.path.exists(dir_path):\n            os.makedirs(dir_path)\n        f = open(os.path.join(cwd,attack+'_test_master_adv',flag_data), 'wb')\n        pickle.dump((x_adv,y_adv), f)\n        f.close()\n        \n    if model_type=='mtd':\n        if copycat == False:\n            dir_path=os.path.join(cwd,attack+'_test')\n        else:\n            dir_path = os.path.join(cwd,attack+'_test_copycat')\n        if not os.path.exists(dir_path):\n            os.makedirs(dir_path)\n        f = open(os.path.join(dir_path,flag_data), 'wb')\n        pickle.dump((x_adv,y_adv), f)\n        f.close()\n        \n    #if model_type=='student':\n    #    dir_path=os.path.join(cwd,attack+'_student_adv')\n    #    if not os.path.exists(dir_path):\n    #        os.makedirs(dir_path)\n        \n    #    f = open(os.path.join(dir_path,flag_data), 'wb')\n    #    pickle.dump((x_adv,y_adv), f)\n    #    f.close()\n    \n    return x_adv, y_adv\n\ndef robustness(model,data,size,batch_size=128,attack='CW',model_type='student',copycat=False):\n    '''Comupte the accuracy under attack'''\n    \n    print('model_type= ',model_type)\n    print('Copycat = ', copycat)\n    if model_type=='master_adv':\n        if copycat==False:\n            path = os.path.join(cwd,attack+'_test_master_adv',flag_data)\n        else:\n            path = os.path.join(cwd,attack+'copycat_test_master_adv',flag_data)\n     \n        \n    elif model_type in ['master','student']:\n        if copycat==False:\n            path = os.path.join(cwd,attack+'_test_master',flag_data)\n        else:\n            path = os.path.join(cwd,attack+'copycat_test_master',flag_data)\n        if attack in ['CW','FGS']:\n            if copycat==False:\n                path = os.path.join(cwd,attack+'_test',flag_data)\n            else:\n                path = os.path.join(cwd,attack+'copycat_test_master',flag_data)\n                \n\n    else:\n        if attack=='spsa':\n            dir_path=os.path.join(cwd,attack+'_test'+''.join(str(flag_lamda).split('.'))+'_'+str(flag_p))\n            if model_type=='mtd' and flag_p>4:\n                dir_path=os.path.join(cwd,attack+'_test'+''.join(str(flag_lamda).split('.'))+'_'+str(5))\n            elif model_type=='mtd' and flag_p<=4:\n                dir_path=os.path.join(cwd,attack+'_test'+''.join(str(flag_lamda).split('.'))+'_'+str(3))\n            \n        else:\n            if copycat==False:\n                dir_path=os.path.join(cwd,attack+'_test')\n            else:\n                dir_path=os.path.join(cwd,attack+'_test_copycat')\n        path = os.path.join(dir_path,flag_data)\n    \n    \n    print(path)\n    if not os.path.exists(path):\n        if model_type in ['master_adv','master'] and copycat==False:\n            if device == \"cuda\":\n                model = model.cuda()\n            model.eval()\n        elif model_type in ['student','mtd']:\n            if copycat==False:\n                master_model = load_model(cwd,'')\n                if device == \"cuda\":\n                    master_model = master_model.cuda()\n                master_model.eval()\n            else:\n                target_model=load_model(os.path.join(cwd,'Copycat','Framework'),'',model_type=model_type,copycat=True)\n                if device == \"cuda\":\n                    target_model = target_model.cuda()\n                target_model.eval()\n        elif model_type in ['master_adv','master'] and copycat==True:\n            target_model=load_model(os.path.join(cwd,'Copycat','Framework'),'',model_type=model_type,copycat=True)\n            if device == \"cuda\":\n                target_model = target_model.cuda()\n            target_model.eval()\n            \n        if attack in ['CW', 'FGS']:\n            if attack=='CW':\n                size=1000\n                    \n            if model_type in ['student','master','mtd']:\n                if copycat==False:\n                    master_model=load_model(cwd,'')\n                    x_adv, y_adv = perform_attack(master_model,data,size,attack=attack,model_type='mtd')\n                else:\n                    x_adv, y_adv = perform_attack(target_model,data,size,attack=attack,model_type=model_type,copycat=True)\n            \n                \n            if model_type in ['master_adv']:\n                if copycat==False:\n                    x_adv, y_adv = perform_attack(model,data,size,attack=attack,model_type=model_type)\n                else:\n                    x_adv, y_adv = perform_attack(target_model,data,size,attack=attack,model_type=model_type,copycat=copycat)\n            \n                    \n        else:\n            x_adv, y_adv = perform_attack(model,data,size,attack=attack,model_type=model_type)\n            \n    \n    else:\n        #print('Loading ', path)\n        f = open(path, 'rb')\n        x_adv,y_adv = pickle.load(f)\n        f.close()\n    \n    if model_type in ['student', 'master', 'master_adv']:\n        if device == \"cuda\":\n            model = model.cuda()\n        model.eval()\n        \n    report = EasyDict(nb_test=0, correct=0)\n    shape=0\n    for i in range(0,x_adv.shape[0],batch_size):\n        x, y = get_batch(x_adv,y_adv, i, batch_size)\n        x, y = x.to(device), y.to(device)\n        _, y_pred = model(x).max(1)\n        report.nb_test += y.size(0)\n        report.correct += y_pred.eq(y).sum().item() \n        shape+=x.shape[0]\n        if model_type=='mtd':\n            print('Current robustness against {} is :{} %'.format(attack,report.correct / report.nb_test * 100.0))\n        del x, y\n        torch.cuda.empty_cache()\n        \n        if shape >= size:\n            break\n        \n    return report.correct / report.nb_test * 100.0\n        \n\ndef get_batch(x_all,y_all, start, batch_size=128):\n    '''Get a batch of data'''\n    \n    y=[]\n    i = start\n    while i<start+batch_size and i<x_all.shape[0]:\n        if i==start:\n            x = x_all[i:i+1]\n            y.append(y_all[i].item())\n        elif i>start:\n            x = torch.cat((x,x_all[i:i+1]))\n            y.append(y_all[i].item())\n        i+=1\n        \n        \n    y=torch.LongTensor(y)\n    \n    \n    return x, y\n            \ndef retrain(net,data,device,epochs,batch_size=128,transform=None,adversarial=False,save=False):\n    '''retrain network either on new clean data or adversarial data'''\n    \n    if device == \"cuda\":\n        net = net.cuda()\n    loss_fn = torch.nn.CrossEntropyLoss(reduction=\"mean\")\n    if flag_data == 'MNIST':\n        optimizer = torch.optim.Adam(net.parameters(), lr=1e-3)\n    elif flag_data == 'CIFAR10':\n        optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n\n    # Train model\n    net.train()\n    for epoch in range(1, epochs + 1):\n        train_loss = 0.0\n        \n        i=0\n        start_time = time.time()\n        with tqdm(data.train, unit=\"batch\") as tepoch:\n            for x, y in tepoch:\n                x, y = x.to(device), y.to(device)\n                #if i%2==0:\n                if transform != None:\n                    x=transform(x)\n                        \n                       \n                if adversarial==True: \n                    \n                    #if i%2==0:\n                    # Replace clean example with adversarial example for adversarial training\n                    master_model = load_model(cwd,'')\n                    '''\n                    if i%100==0 and flag_data=='MNIST':\n                        #print('performing CW on batch {} for adversarial training'.format(i)) \n                        x = carlini_wagner_l2(master_model, x, 10,y,targeted = False)\n                    '''\n                    #else: \n                    #print('performing PGD on batch {} for adversarial training'.format(i))\n                    x = projected_gradient_descent(master_model, x, flag_eps, 0.01, 40, np.inf)\n                            \n                            \n                        \n                \n                optimizer.zero_grad()\n                loss = loss_fn(net(x), y)\n                loss.backward()\n                optimizer.step()\n                train_loss += loss.item()\n                    \n                i+=1\n                \n        #print(\"--- A  epoch takes %s seconds ---\" % (time.time() - start_time))\n        '''print(\"epoch: {}/{}, train loss: {:.3f}\".format(\n                epoch, epochs, train_loss))'''    \n            \n\n    acc = accuracy(net,data,flag_test_set)\n                                                                                           \n    return net, acc\n\n\ndef perturb_weights_and_retrain(master_path,data,lamda,n,p,batch_size=128,new_train=False):\n    \n    '''Generate n diverse and accurate student models where\n        p of the n models are adversarially trained\n        \n    :param master_path: path to the state_dict of master model\n    :param data: test data loader\n    :param lamda: the exponential decay of laplacian noise\n    :param n: number of student models to generate\n    :param p: number of adversarially trained student models\n    '''\n    \n    for i in range(n):\n        print('## Generating student model {} ##'.format(i+1))\n        \n        # load master model\n        model = load_model(os.path.join(master_path),'')\n        master_acc = accuracy(model,data,flag_test_set)\n        print(\"test acc of master model (%): {:.3f}\".format(master_acc))\n        rep=0\n        while True:\n            model = load_model(os.path.join(master_path),'')\n            \n        \n            for param_tensor in model.state_dict():\n                \n                shape = model.state_dict()[param_tensor].size()\n                \n                # laplace mechanism\n                if device == \"cuda\":\n                    try:\n                        model.state_dict()[param_tensor]+= torch.cuda.FloatTensor(np.random.laplace(loc=0.0, scale=lamda, size=shape))\n                    except RuntimeError:\n                        model.state_dict()[param_tensor]+= torch.cuda.LongTensor(np.random.laplace(loc=0.0, scale=lamda, size=shape))\n                else:\n                    model.state_dict()[param_tensor]+= np.random.laplace(loc=0.0, scale=lamda, size=shape)\n            acc = accuracy(model,data,flag_test_set)\n            rep+=1\n            if acc>10:\n                break\n            if rep==5:\n                print('student model is not retrainable please try noise scale lower than {}'.format(flag_lamda))\n                #pass\n                return\n        print(\"Acc of student model {} after weight perturbation (%): {:.3f}\".format(i+1,acc))\n        \n        # retrain student model\n        trans_shift = 0.1+(random()*(0.2-0.1)) #scaled value = min + (value * (max - min))\n        rot_deg = 10+(random()*(20-10))\n        transform=torchvision.transforms.RandomAffine(degrees=rot_deg, translate=(trans_shift,trans_shift))\n        \n        \n        epoch=0\n        while True:\n            if epoch%5==0:\n            #if epoch%2==0:\n                old_acc=acc\n            epoch+=1\n            if new_train==False:\n                model, acc = retrain(model,data,device,1,batch_size=flag_batch,transform=None,adversarial=False)\n            else:\n                model, acc = retrain(model,data,device,1,batch_size=flag_batch,transform=transform,adversarial=False)\n            if acc < old_acc:\n                old_acc=acc\n            print(\"Accuracy of student model after {} epochs of retraining (%): {:.3f}\".format(epoch,acc))\n            if epoch%5==0:\n            #if epoch%2==0:\n                print('Old_acc :',old_acc)\n                if acc >= np.floor(master_acc) or acc-old_acc<0.5:\n                    break\n     \n    \n        print(\"Acc of student model {} after retraining (%): {:.3f}\".format(i+1,acc))\n        \n        if i >= n-p:\n            old_acc = acc\n            start_time = time.time()\n            it = 0\n            #if n-i <= p:\n            print('# Performing adversarial training on student model {}'.format(i+1))\n            epoch=0\n            old_rob = robustness(model,data,1000,batch_size,attack='FGS')\n            max_rob = old_rob\n            print(\"Robustness of student model {} before adversarial training (%) is {:.3f}\".format(i+1,old_rob))\n            while True:\n                epoch+=1\n                model, acc = retrain(model,data,device,1,batch_size=flag_batch,transform=transform,adversarial=True)\n                rob = robustness(model,data,1000,batch_size,attack='FGS')\n                it +=1\n                if rob > max_rob:\n                    max_rob = rob\n                if rob < old_rob:\n                    old_rob=rob\n                if epoch%7==0:\n                    print('old_rob',old_rob)\n                    if rob-old_rob<1:\n                        break\n                    old_rob=rob\n                if it >= flag_max_iter and rob >=max_rob:\n                    break\n                \n             \n                print(\"Robustness of student model after {} epochs of adversarial training (%) is {:.3f} with acc={:.3f}\".format(epoch,rob,acc))\n            \n            print(\"Acc of student model {} after retraining (%): {:.3f}\".format(i+1,acc))\n            print(\"--- Adversarial training takes %s seconds ---\" % (time.time() - start_time))\n        # Save student models\n        if new_train==False:\n            dir_path = os.path.join(cwd,'experiments', flag_data, flag_data+\"_models_\"+''.join(str(flag_lamda).split('.'))+'_'+str(flag_n)+flag_models_batch+'Xt')\n        else:\n            dir_path = os.path.join(cwd,'experiments', flag_data,flag_data+\"_models_\"+''.join(str(flag_lamda).split('.'))+'_'+str(flag_n)+'_'+flag_models_batch)\n        print(dir_path)\n        if not os.path.exists(dir_path):\n            os.makedirs(dir_path)\n        filename = os.path.join(dir_path,\"CNN_\"+flag_data + str(i+1) + \".pth\")\n        torch.save(model.state_dict(),filename)\n        \n    \ndef predict1(x):\n    ''' predict the labels of a set x using conf-weighted scheduling of MTD'''\n\n    models=[]\n    for i in range(1,flag_n+1):\n        models.append(load_model(os.path.join(cwd,flag_data+\"_models_\"+''.join(str(flag_lamda).split('.'))+'_'+str(flag_p)+'_'+flag_models_batch),i))\n    \n    i=0\n    y_probs=[]\n    for model in models:\n        if device == \"cuda\":\n            model = model.cuda()\n        model.eval()\n        x=x.to(device)\n        \n        if i==0:\n            y_probs = model(x)\n        else:\n            y_probs+= model(x)\n        \n        i+=1\n    \n    \n    return y_probs\n\n\nclass Morphence():\n    \n    '''Morphence prediction system'''\n    \n    def __init__(self, test_size, Q_max,n,data,lamda,starting_batch, class_nb):\n        \n        self.test_size = test_size\n        self.Q_max = Q_max # Mak\n        self.n = n\n        self.data = data\n        self.lamda = lamda\n        self.starting_batch = starting_batch\n        self.class_nb = class_nb\n        \n        self.nb_queries = 0 # total number of queries previously performed on Morphence\n        self.scheduling={} # number of selections for each model\n        for i in range(self.n+1): \n            self.scheduling[i]=0\n        \n        # distribution of test set over different pool of models\n        self.queries = list(range(0,self.test_size+1,self.Q_max)) # queries limits for each pool of models\n        \n        if self.test_size % self.Q_max != 0:\n            self.queries.append(self.test_size % self.Q_max + self.queries[-1])\n        \n        if self.Q_max<=5000:\n            print('The distribution of {} queries with respect to Q_max = {} is {}.'.format(self.test_size,self.Q_max,self.queries))\n        \n    def predict2(self,x):\n        ''' predict the labels of a set x using highest conf scheduling of MTD'''\n        \n        #print(self.nb_queries)\n        print('Received {} queries'.format(x.shape[0]))\n        \n        # input transformations\n        #trans_shift = 0.1+(random()*(0.25-0.1)) #scaled value = min + (value * (max - min))\n        #rot_deg = 10+(random()*(20-10))\n        #transform=torchvision.transforms.RandomAffine(degrees=0, translate=(trans_shift,trans_shift))\n        #x=transform(x)\n        \n        # Gaussian Noise \n        #x = x + np.sqrt(0.1)*(0.1**0.5)*torch.randn(x.shape).to(device)\n        \n        y_probs=[] # prediction probabilities  of all models\n        \n        for qi in range(len(self.queries)-1):\n            \n            if self.nb_queries >= self.queries[qi] and self.nb_queries < self.queries[qi+1]:\n                if self.nb_queries + x.shape[0] <= self.queries[qi+1]:\n                    models=[]\n                    for i in range(1,self.n+1):\n                        try:\n                            #models.append(load_model(os.path.join(cwd,self.data+\"_models_\"+''.join(str(self.lamda).split('.'))+'_'+str(flag_p)),i))\n                            models.append(load_model(os.path.join(cwd,'experiments',self.data,self.data+\"_models_\"+''.join(str(self.lamda).split('.'))+'_'+str(self.n)+'_'+self.starting_batch[0]+str(int(self.starting_batch[1])+qi)),i))\n                        except FileNotFoundError:\n                            raise('model {} is not found'.format(i))\n                            \n                    print('### Responding to queries from {} to {} using models pool {}'.format(self.nb_queries+1,self.nb_queries + x.shape[0],self.starting_batch[0]+str(int(self.starting_batch[1])+qi)))\n                    for model in models:\n                        if device == \"cuda\":\n                            model = model.cuda()\n                        model.eval()\n                        y_probs.append(model(x))\n                \n                elif self.nb_queries + x.shape[0] > self.queries[qi+1]:\n                    models1=[]\n                    models2=[]\n                    x1 = x[:self.queries[qi+1]-self.nb_queries]\n                    print('### Responding to queries from {} to {} using models pool {}'.format(self.nb_queries+1,self.queries[qi+1],self.starting_batch[0]+str(int(self.starting_batch[1])+qi)))\n                    if self.queries[qi+1] < self.test_size:\n                        x2 = x[self.queries[qi+1]-self.nb_queries:]\n                        print('And responding to queries from {} to {} using models pool {}'.format(self.queries[qi+1]+1,self.nb_queries + x.shape[0],self.starting_batch[0]+str(int(self.starting_batch[1])+qi+1)))\n                    for i in range(1,self.n+1):\n                        try:\n                            models1.append(load_model(os.path.join(cwd,'experiments',self.data,self.data+\"_models_\"+''.join(str(self.lamda).split('.'))+'_'+str(self.n)+'_'+self.starting_batch[0]+str(int(self.starting_batch[1])+qi)),i))\n                        except FileNotFoundError:\n                            raise('model {} is not found'.format(i))\n                            \n                        if self.queries[qi+1] < self.test_size:\n                            try:\n                                models2.append(load_model(os.path.join(cwd,'experiments',self.data,self.data+\"_models_\"+''.join(str(self.lamda).split('.'))+'_'+str(self.n)+'_'+self.starting_batch[0]+str(int(self.starting_batch[1])+qi+1)),i))\n                            except FileNotFoundError:\n                                raise('model {} is not found'.format(i))\n                                \n                    if self.queries[qi+1] < self.test_size:\n                        for model1,model2 in zip(models1,models2):\n                            if device == \"cuda\":\n                                model1 = model1.cuda()\n                                model2 = model2.cuda()\n                            model1.eval()\n                            model2.eval()\n                            \n                            y_probs.append(torch.cat((model1(x1),model2(x2)),dim=0))\n                    else:\n                        \n                        for model1 in models1:\n                            if device == \"cuda\":\n                                model1 = model1.cuda()\n                                \n                            model1.eval()\n                            y_probs.append(model1(x))\n                            \n        \n            \n        \n        # update number of queries                       \n        self.nb_queries += x.shape[0]\n        \n        # Select the model that has the highest prediction confidence\n        for ind in range(x.shape[0]):\n            for i in range(self.n):\n                \n                if i==0:\n                    y_ind = torch.reshape(y_probs[i][ind], (1, self.class_nb))\n                else:\n                    y_ind = torch.cat((y_ind,torch.reshape(y_probs[i][ind], (1, self.class_nb))),dim=0)\n                \n            #print('predictions of sample {} are {}'.format(ind,y_ind))\n            #print('highest confidence vector of sample {} are {}'.format(ind, y_ind.max(0)[0]))\n            \n            \n            #print('top k highest confidence vector of sample {} are {}'.format(ind, torch.topk(y_ind,5,dim=0)[0]))\n            \n            #print(y_ind.max(0))\n            #print(y_ind.max(0)[0].argmax().item())\n            selected_ind = y_ind.max(0)[1][y_ind.max(0)[0].argmax().item()].item()\n            \n            #if flag_data=='CIFAR10' and selected_ind in [0,1,2]:\n            #    selected_ind = torch.topk(y_ind,2,dim=0)[1][1][torch.topk(y_ind,2,dim=0)[0][1].argmax().item()]\n            #print('selected model for sample {} is model {}'.format(ind, selected_ind))\n            \n            # keep track of selected models\n            self.scheduling[selected_ind]+=1\n            \n            if ind == 0:\n                y_pred = torch.reshape(y_ind.max(0)[0], (1, self.class_nb))\n            else:\n                y_pred = torch.cat((y_pred,torch.reshape(y_ind.max(0)[0], (1, self.class_nb))))\n                \n            \n        \n        \n        return y_pred\n\ndef transferability(attack,data,size,batch_size=128):\n    '''funkcja liczy odporność na przenoszenie ataków pomiedzy modelami'''\n    \n    transf=[] # list of average transferabilities for each student model\n    models=[]\n    for i in range(1,flag_n+1):\n        path = os.path.join(cwd,'experiments', data,data+\"_models_\"+''.join(str(flag_lamda).split('.'))+'_'+str(flag_n)+'_'+flag_models_batch)\n        models.append(load_model(path,i))\n    \n    data = load_data()\n    for i in range(len(models)):\n        if device == \"cuda\":\n            models[i] = models[i].cuda()\n        models[i].eval()\n        \n        print('performing {} attack on model {}'.format(attack,i+1))\n        x_adv, y_adv = perform_attack(models[i],data,size,attack=attack,model_type='student')\n        \n        transfi=[] # transferability of model i across all student models using using all adv data\n        for j in range(len(models)):\n            if j != i:\n                if device == \"cuda\":\n                    models[j] = models[j].cuda()\n                models[j].eval()\n                \n                tot=0 # total of adv samples on model i\n                s=0 # sum of transferable samples for model j\n                for b_i in range(0,x_adv.shape[0],batch_size):\n                    x, y = get_batch(x_adv,y_adv, b_i, batch_size)\n                    x, y = x.to(device), y.to(device)\n                    _, y_predi = models[i](x).max(1)\n                    resi = y_predi.eq(y)\n                    \n                    _, y_predj = models[j](x).max(1)\n                    resj = y_predj.eq(y)\n                    \n                    for ind in range(resi.shape[0]):\n                        if resi[ind] == False: # evasion on model i\n                            tot+=1\n                            if resj[ind] == False: # transferable to model j\n                                s+=1\n                print('Transferability of model {} to model {}: {}'.format(i+1,j+1,float(s)/tot))\n                transfi.append(float(s)/tot)\n        transf.append(np.mean(transfi))\n        print('Avergae Transferability of model {} across all models: {}'.format(i+1,np.mean(transfi)))\n        \n    print('Overall transferability of MTD framework using {} attack: {}'.format(attack,np.mean(transf)))\n    \n    return np.mean(transf)\n\ndef test_under_attack(model,data,size,attack='CW',batch_size=128,model_type='mtd',copycat=False):\n    # Sprawdzenie odporności podczas ataku\n    return robustness(model,data,size,batch_size=batch_size,attack=attack,model_type=model_type,copycat=copycat)\n\ndef generate_students():\n    '''Ładowanie zbioru danych poczym wykonuje się perturbacje wag i trening modeli stundetów'''\n    print(f'flag_models_batch is {flag_models_batch}')\n    print(\"/*** Generating a batch of student models ***/\\n\")\n    data = load_data()\n    perturb_weights_and_retrain(cwd,data,flag_lamda,flag_n,flag_p,batch_size=flag_batch,new_train=True)\n\ndef test_base():\n    \n    print('Loading data ...')\n    data = load_data()#train=False\n    \n    # Ładowanie modelu oryginalnego\n    # Funkcja test_base była używana do testowania jak niebroniony model radzi sobie z atakami.\n    master_model=load_model(cwd,'')\n    \n    if flag_attack=='NoAttack':\n        print('Acc of master model ',accuracy(master_model,data,flag_test_set,model_type='master'))\n    else:\n        print('Acc of master model under {} attack {}'.format(flag_attack,test_under_attack(master_model,data,flag_test_set,attack=flag_attack,model_type='master',copycat=False)))\n    \n\ndef test_adv():\n    \n    print('Loading data ...')\n    data = load_data()#train=False\n    \n    # Ładowanie modelu oryginalnego\n    # Funkcja test_adv była używana do testowania jak model radzi sobie z atakami gdy był trenowany z użyciem przypadków przeciwstawnych.\n    model=load_model(cwd,'')\n    \n    old_acc = accuracy(model,data,flag_test_set,model_type='master')\n    start_time = time.time()\n    it = 0\n    #if n-i <= p:\n    print('# Performing adversarial training')\n    epoch=0\n    old_rob = robustness(model,data,1000,batch_size=flag_batch,attack='FGS')\n    max_rob = old_rob\n    print(\"Robustness before adversarial training (%) is {:.3f}\".format(old_rob))\n    while True:\n        epoch+=1\n        model, acc = retrain(model,data,device,1,batch_size=flag_batch,adversarial=True)\n        rob = robustness(model,data,1000,batch_size=flag_batch,attack='FGS')\n        it +=1\n        if rob > max_rob:\n            max_rob = rob\n        if rob < old_rob:\n            old_rob=rob\n        if epoch%5==0:\n            print('old_rob',old_rob)\n            if rob-old_rob<1:\n                break\n            old_rob=rob\n        if it >= 50 and rob >=max_rob:\n            break\n    \n    \n    if flag_attack=='NoAttack':\n        print('Acc of adversarially trained model ',accuracy(model,data,flag_test_set,model_type='master_adv'))\n    else:\n        print('Acc of adversarially trained model under {} attack {}'.format(flag_attack,test_under_attack(model,data,flag_test_set,attack=flag_attack,model_type='master_adv',copycat=False)))\n        \ndef test():\n\n    if flag_Q_max>5000:\n        raise('Q_max is higher than the test set size. use a lower value')\n        \n    if flag_attack not in supported_attacks:\n        raise('attack is not supported try CW, FGS or SPSA')\n    print('Loading data ...')\n    data = load_data()#train=False\n    \n    if flag_attack == 'SPSA':\n        dir_path=os.path.join(cwd,flag_attack+'_test')\n        if not os.path.exists(dir_path):\n            mtd_inst = Morphence(6000*6000, 6000*6000,flag_n,flag_data,flag_lamda,flag_models_batch, flag_class_nb)\n            perform_attack(mtd_inst.predict2,data,flag_test_set,attack='spsa',model_type='mtd',copycat=False)\n         \n    print('Initializing Morphence ...')\n    mtd_inst = Morphence(flag_test_set, flag_Q_max,flag_n,flag_data,flag_lamda,flag_models_batch, flag_class_nb)\n    \n    if flag_attack=='NoAttack':\n        print('Acc of MTD framework before attack',accuracy(mtd_inst.predict2,data,flag_test_set,model_type='mtd'))\n    else:\n        print('Acc of MTD framework under {} attack {}'.format(flag_attack, test_under_attack(mtd_inst.predict2,data,flag_test_set,attack=flag_attack,batch_size=flag_batch,model_type='mtd',copycat=False)))  ","metadata":{"id":"F_G1rrdSK3iM","execution":{"iopub.status.busy":"2022-06-13T09:20:23.989664Z","iopub.execute_input":"2022-06-13T09:20:23.990036Z","iopub.status.idle":"2022-06-13T09:20:25.200999Z","shell.execute_reply.started":"2022-06-13T09:20:23.990002Z","shell.execute_reply":"2022-06-13T09:20:25.199795Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Funkcje do treningu modelu 4 tyś. parametrów\n\n","metadata":{"id":"XYoPELYpK3ij"}},{"cell_type":"code","source":"from absl import app, flags\nfrom easydict import EasyDict\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nfrom torchvision.datasets import MNIST\nimport torch.optim as optim\n\nfrom cleverhans.torch.attacks.fast_gradient_method import fast_gradient_method\nfrom cleverhans.torch.attacks.projected_gradient_descent import (\n    projected_gradient_descent,\n)\n\nimport os\n\nclass Model4k(nn.Module):\n    def __init__(self, in_channels = 1):\n        super(Model4k,self).__init__()\n        self.conv1 = nn.Conv2d(in_channels=1,out_channels=8,kernel_size=5)\n        self.avgpol1 = nn.AvgPool2d(2, stride=2)\n        self.conv2 = nn.Conv2d(in_channels=8,out_channels=4,kernel_size=5)\n        self.avgpol2 = nn.AvgPool2d(2, stride=2)\n          \n        self.fc1 = nn.Linear(in_features=4*4*4,out_features=36)\n        self.fc2 = nn.Linear(in_features=36,out_features=10)\n        \n        self.drop1 = nn.Dropout(p=0.25)\n        self.drop2 = nn.Dropout(p=0.15)\n      \n    def forward(self, x):    \n        #first hidden layer\n        x = self.conv1(x)\n        x = F.relu(x)\n        x = self.drop1(x)\n        x = self.avgpol1(x)\n        \n        x = self.conv2(x)\n        x = F.relu(x)\n        x = self.drop2(x)\n        x = self.avgpol2(x)\n        \n        x = x.view(-1,4*4*4)\n        \n        x = self.fc1(x)\n        x = self.fc2(x)\n        \n        output = F.log_softmax(x, dim=1)\n        return output\n\n\ndef ld_mnist(batch_size=128, transform=None,shuffle=True):\n    \"\"\"Load training and test data.\"\"\"\n    \n    if transform==None:\n        train_transforms = torchvision.transforms.Compose(\n            [torchvision.transforms.ToTensor()]\n        )\n        test_transforms = torchvision.transforms.Compose(\n            [torchvision.transforms.ToTensor()]\n        )\n    else:\n        train_transforms = transform\n        \n        test_transforms = torchvision.transforms.Compose(\n            [torchvision.transforms.ToTensor()]\n        )\n\n    # Load MNIST dataset\n    train_dataset = MNIST(root='./data', train=True, download=True, transform=train_transforms)\n    test_dataset = MNIST(root='./data', train=True, download=True, transform=test_transforms)\n\n    train_loader = torch.utils.data.DataLoader(\n        train_dataset, batch_size=batch_size, shuffle=shuffle, num_workers=2\n    )\n    test_loader = torch.utils.data.DataLoader(\n        test_dataset, batch_size=batch_size, shuffle=False, num_workers=2\n    )\n    return EasyDict(train=train_loader, test=test_loader)\n\n\ndef train_Model4k():\n    # Load training and test data\n    data = ld_mnist()\n\n    # Instantiate model, loss, and optimizer for training\n    net = Model4k(in_channels=1)\n\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    if device == \"cuda\":\n        net = net.cuda()\n    loss_fn = torch.nn.CrossEntropyLoss(reduction=\"mean\")\n    \n    optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.9)\n    best_loss = 1000\n    patience = 5\n    # Train model\n    net.train()\n    for epoch in range(1, flag_nb_epochs + 1):\n        train_loss = 0.0\n        for x, y in data.train:\n            x, y = x.to(device), y.to(device)\n            '''\n            if flag_adv_train:\n                # Replace clean example with adversarial example for adversarial training\n                x = projected_gradient_descent(net, x, flag_eps, 0.01, 40, np.inf)\n            '''\n            optimizer.zero_grad()\n            loss = loss_fn(net(x), y)\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item()\n        # Early stopping\n        if train_loss < best_loss:\n            best_loss = train_loss\n            trigger_times = 0\n        else:\n            trigger_times += 1\n            print('Trigger Times:', trigger_times)\n\n            if trigger_times >= patience:\n                print('Early stopping!\\nStart to test process.')\n                break\n\n        last_loss = train_loss           \n        print(\n            \"epoch: {}/{}, train loss: {:.3f}\".format(\n                epoch, flag_nb_epochs, train_loss\n            )\n        )\n\n    # Evaluate on clean and adversarial data\n    net.eval()\n    report = EasyDict(nb_test=0, correct=0, correct_fgm=0, correct_pgd=0)\n    for x, y in data.test:\n        x, y = x.to(device), y.to(device)\n        \n        _, y_pred = net(x).max(1)  # model prediction on clean examples\n        report.nb_test += y.size(0)\n        report.correct += y_pred.eq(y).sum().item()\n        \n        \n    print(\"test acc on clean examples (%): {:.3f}\".format(report.correct / report.nb_test * 100.0))\n    \n    # save model\n    filename = os.path.join(cwd,\"CNN_MNIST.pth\")\n    torch.save(net.state_dict(),filename)","metadata":{"id":"xE-VjGiwK3im","execution":{"iopub.status.busy":"2022-06-13T09:20:25.203006Z","iopub.execute_input":"2022-06-13T09:20:25.204181Z","iopub.status.idle":"2022-06-13T09:20:25.257752Z","shell.execute_reply.started":"2022-06-13T09:20:25.204130Z","shell.execute_reply":"2022-06-13T09:20:25.254233Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Funkcje do treningu modelu 12 tyś. parametrów","metadata":{"id":"Fo5oZJFmK-Fz"}},{"cell_type":"code","source":"from absl import app, flags\nfrom easydict import EasyDict\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nfrom torchvision.datasets import MNIST\nimport torch.optim as optim\n\nfrom cleverhans.torch.attacks.fast_gradient_method import fast_gradient_method\nfrom cleverhans.torch.attacks.projected_gradient_descent import (\n    projected_gradient_descent,\n)\n\nimport os\nFLAGS = flags.FLAGS\n\n# Get current working directory\ncwd = os.getcwd()\n\nclass Model12k(nn.Module):\n    def __init__(self, in_channels=1):\n        super(Model12k, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, 10, kernel_size=5)\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n        self.conv2_drop = nn.Dropout2d(p=0.15)\n        self.fc1 = nn.Linear(320, 20)\n        self.fc2 = nn.Linear(20, 10)\n\n    def forward(self, x):\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n        x = x.view(-1, 320)\n        x = F.relu(self.fc1(x))\n        x = F.dropout(x, training=self.training)\n        x = self.fc2(x)\n        return F.log_softmax(x)\n\n\ndef ld_mnist(batch_size=128, transform=None,shuffle=True):\n    \"\"\"Load training and test data.\"\"\"\n    \n    if transform==None:\n        train_transforms = torchvision.transforms.Compose(\n            [torchvision.transforms.ToTensor()]\n        )\n        test_transforms = torchvision.transforms.Compose(\n            [torchvision.transforms.ToTensor()]\n        )\n    else:\n        train_transforms = transform\n        \n        test_transforms = torchvision.transforms.Compose(\n            [torchvision.transforms.ToTensor()]\n        )\n\n    # Load MNIST dataset\n    train_dataset = MNIST(root='./data', train=True, download=True, transform=train_transforms)\n    test_dataset = MNIST(root='./data', train=True, download=True, transform=test_transforms)\n\n    train_loader = torch.utils.data.DataLoader(\n        train_dataset, batch_size=batch_size, shuffle=shuffle, num_workers=2\n    )\n    test_loader = torch.utils.data.DataLoader(\n        test_dataset, batch_size=batch_size, shuffle=False, num_workers=2\n    )\n    return EasyDict(train=train_loader, test=test_loader)\n\n\ndef train_Model12k():\n    # Load training and test data\n    data = ld_mnist(256)\n\n    # Instantiate model, loss, and optimizer for training\n    net = Model12k(in_channels=1)\n\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    if device == \"cuda\":\n        net = net.cuda()\n    loss_fn = torch.nn.CrossEntropyLoss(reduction=\"mean\")\n    \n    #optimizer = optim.Adadelta(net.parameters(), lr=1.0)\n    #optimizer = optim.SGD(net.parameters(), lr=1.0, momentum=0.7)\n    optimizer = torch.optim.Adam(net.parameters(), lr=1e-2)\n    #optimizer = torch.optim.NAdam(net.parameters(), lr=1e-2)\n    # Train model\n    net.train()\n    for epoch in range(1, flag_nb_epochs + 1):\n        train_loss = 0.0\n        for x, y in data.train:\n            x, y = x.to(device), y.to(device)\n            '''\n            if FLAGS.adv_train:\n                # Replace clean example with adversarial example for adversarial training\n                x = projected_gradient_descent(net, x, FLAGS.eps, 0.01, 40, np.inf)\n            '''\n            optimizer.zero_grad()\n            loss = loss_fn(net(x), y)\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item()\n        print(\n            \"epoch: {}/{}, train loss: {:.3f}\".format(\n                epoch, flag_nb_epochs, train_loss\n            )\n        )\n\n    # Evaluate on clean and adversarial data\n    net.eval()\n    report = EasyDict(nb_test=0, correct=0, correct_fgm=0, correct_pgd=0)\n    for x, y in data.test:\n        x, y = x.to(device), y.to(device)\n        \n        _, y_pred = net(x).max(1)  # model prediction on clean examples\n        report.nb_test += y.size(0)\n        report.correct += y_pred.eq(y).sum().item()\n        \n        \n    print(\"test acc on clean examples (%): {:.3f}\".format(report.correct / report.nb_test * 100.0))\n    \n    # save model\n    filename = os.path.join(cwd,\"CNN_MNIST.pth\")\n    torch.save(net.state_dict(),filename)\n","metadata":{"id":"PGaeQ4uBK9Kq","execution":{"iopub.status.busy":"2022-06-13T09:30:09.201954Z","iopub.execute_input":"2022-06-13T09:30:09.202346Z","iopub.status.idle":"2022-06-13T09:30:09.237962Z","shell.execute_reply.started":"2022-06-13T09:30:09.202314Z","shell.execute_reply":"2022-06-13T09:30:09.236554Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# Ustawienie niezbędnych parametrów","metadata":{"id":"D8KIGWzOK3io"}},{"cell_type":"code","source":"flag_lamda = 0.1\nflag_max_iter  = 10\nflag_batch = 1024\nflag_eps = 0.3\nflag_data = 'MNIST'\nflag_p = 3\nflag_n = 4\nflag_test_set = 5000\n","metadata":{"id":"XyUWxvLwK3ip","execution":{"iopub.status.busy":"2022-06-13T09:30:11.604074Z","iopub.execute_input":"2022-06-13T09:30:11.604570Z","iopub.status.idle":"2022-06-13T09:30:11.611532Z","shell.execute_reply.started":"2022-06-13T09:30:11.604538Z","shell.execute_reply":"2022-06-13T09:30:11.610035Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# Trening modelu 4k","metadata":{"id":"jXfOBUnhK3iq"}},{"cell_type":"code","source":"flag_nb_epochs = 10\ntrain_Model4k()","metadata":{"id":"uG7LLZrxK3iq","execution":{"iopub.status.busy":"2022-06-13T09:31:41.456046Z","iopub.execute_input":"2022-06-13T09:31:41.456678Z","iopub.status.idle":"2022-06-13T09:33:21.618386Z","shell.execute_reply.started":"2022-06-13T09:31:41.456639Z","shell.execute_reply":"2022-06-13T09:33:21.617112Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"# Trening modelu 12k","metadata":{"id":"AHliaWdbMSia"}},{"cell_type":"code","source":"flag_nb_epochs = 7\ntrain_Model12k()","metadata":{"id":"lCdmtlmcMTMb","execution":{"iopub.status.busy":"2022-06-13T09:30:16.809172Z","iopub.execute_input":"2022-06-13T09:30:16.809623Z","iopub.status.idle":"2022-06-13T09:31:20.580125Z","shell.execute_reply.started":"2022-06-13T09:30:16.809590Z","shell.execute_reply":"2022-06-13T09:31:20.578413Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"# Generowanie studentów","metadata":{"id":"toBqU67NK3ir"}},{"cell_type":"code","source":"for i in ['b1', 'b2','b3', 'b4', 'b5']:\n    flag_models_batch = i\n    generate_students()","metadata":{"id":"PBvm5N-3K3ir","execution":{"iopub.status.busy":"2022-06-13T09:33:26.477211Z","iopub.execute_input":"2022-06-13T09:33:26.478123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Wykonanie pomiarów dokładności precyzji dla ataków FGS i CW oraz porównanie dokładności predykcji w sytuacji bez ataku (Wykorzystując metodę Morphence)","metadata":{"id":"kYlkoJZQK3is"}},{"cell_type":"code","source":"flag_Q_max = 1000\nflag_class_nb = 10\nflag_models_batch = 'b1'\nfor i in ['FGS', 'CW', 'NoAttack',]:\n    print(\"---------------------------------------------------------------\")\n    flag_attack = i\n    if(flag_attack == 'FGS'):\n        flag_batch = 500\n    else:\n        flag_batch = 1024\n    test()","metadata":{"id":"VCOfvQENK3is","execution":{"iopub.status.busy":"2022-06-13T09:20:25.516520Z","iopub.status.idle":"2022-06-13T09:20:25.517482Z","shell.execute_reply.started":"2022-06-13T09:20:25.517143Z","shell.execute_reply":"2022-06-13T09:20:25.517173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Pomiar transferability","metadata":{"id":"JC1sfifJK3it"}},{"cell_type":"code","source":"for i in ['b1', 'b2','b3', 'b4', 'b5']:\n    flag_models_batch = i\n    transferability(attack = 'FGS',data = 'MNIST',size = 5000,batch_size=1024)","metadata":{"id":"ejbCOv4aK3it","execution":{"iopub.status.busy":"2022-06-13T09:20:25.519061Z","iopub.status.idle":"2022-06-13T09:20:25.519957Z","shell.execute_reply.started":"2022-06-13T09:20:25.519662Z","shell.execute_reply":"2022-06-13T09:20:25.519693Z"},"trusted":true},"execution_count":null,"outputs":[]}]}